\documentclass{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{breqn}
\usepackage{amsthm}
\theoremstyle{definition}
\usepackage{bbm}
\renewcommand{\footnotesize}{\normalsize}
\usepackage{latexsym}
\usepackage{color}
\usepackage{emathEy}
\usepackage{listings,plistings}
\usepackage{bm}
%\newtheorem{Ex}{演習問題}
%\newtheorem*{Ex*}{演習問題}
\newtheoremstyle{mystyle}%   % スタイル名
    {}%                      % 上部スペース
    {}%                      % 下部スペース
    {\normalfont}%           % 本文フォント
    {}%                      % インデント量
    {\bf}%                   % 見出しフォント
    {}%                      % 見出し後の句読点, '.'
    { }%                     % 見出し後のスペース, ' ' or \newline
    {\underline{\thmname{#1}\thmnumber{#2}\thmnote{（#3）}}}%
                             % 見出しの書式 (can be left empty, meaning `normal')
\theoremstyle{mystyle} % スタイルの適用
\newtheorem*{Def}{Def}
\newtheorem*{theo}{Theorem}
\newtheorem*{lem}{Lemma}
\newtheorem*{ex}{Example}
\newtheorem*{col}{Corollary}
\renewcommand{\footnotesize}{\normalsize}
\usepackage{latexsym}
\usepackage{emathEy}
\def\qed{\hfill$\Box$}
\lstset{
language={Python},
backgroundcolor={\color[gray]{.85}},
basicstyle={\footnotesize},
identifierstyle={\footnotesize},
commentstyle={\footnotesize\ttfamily \color[rgb]{0,0.5,0}},
keywordstyle={\footnotesize\bfseries \color[rgb]{1,0,0}},
ndkeywordstyle={\footnotesize},
stringstyle={\footnotesize\ttfamily \color[rgb]{0,0,1}},
frame={tb},
breaklines=true,
columns=[l]{fullflexible},
numbers=left,
xrightmargin=0zw,
xleftmargin=3zw,
numberstyle={\scriptsize},
stepnumber=1,
numbersep=1zw,
morecomment=[l]{//}
}

\begin{document}
\Large
\section*{2.1}
この節では，本章の準備として$W\in\mathbb{R}^{N\times N}$を正定値行列として
\begin{align}
\label{eru}
L_0:=\frac{1}{2N}(y-\beta_0\cdot \mathbbm{1}-X\beta)^T W(y-\beta_0\cdot \mathbbm{1}-X\beta)
\end{align}
の線形回帰のLasso解を求める．そのために，第1章での線形回帰同様$X$の各列及び$y$を中心化して，$\beta_0$を$0$にする．具体的に$i\in\{1,\cdots N\}$に対して重み$W=(w_{i,j})$を考慮した中心化
\begin{align*}
\bar{X}_k&:=\frac{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}x_{j,k}}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}\\
\bar{y}&:=\frac{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}y_j}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}\\
x_{i,k}&\gets x_{i,k}-\bar{X}_k\\
y_i&\gets y_i-\bar{y}
\end{align*}
を行うと，各$k$に対して
\begin{align*}
\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}\left(x_{j,k}-\bar{X}_k\right)&=\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}x_{j,k}-\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}\bar{X}_k\\
&=\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}x_{j,k}-\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}\cdot \frac{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}x_{j,k}}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}=0\\
\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}(y_j-\bar{y})&=\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}y_j-\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}\cdot \frac{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}y_j}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}=0
\end{align*}
が成立することから$\displaystyle\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}x_{j,k}=\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}y_j=0$が成立し，さらに正定値$W$が特に対称行列であることを考えれば
\begin{align*}
\frac{\partial L_0}{\partial \beta_0\cdot\mathbbm{1}}&=\frac{\partial (y-\beta_0\cdot\mathbbm{1}-X\beta)}{\partial \beta_0\cdot\mathbbm{1}}\frac{\partial L_0}{\partial(y-\beta_0\cdot\mathbbm{1}-X\beta)}\\
&=\frac{1}{2N}\cdot \frac{\partial (y-\beta_0\cdot\mathbbm{1}-X\beta)}{\partial \beta_0\cdot\mathbbm{1}}(W+W^T)(y-\beta_0\cdot\mathbbm{1}-X\beta)\\
&=-\frac{1}{N}W(y-\beta_0\cdot\mathbbm{1}-X\beta)
\end{align*}
がわかり，

\begin{align*}
y-\beta_0\cdot\mathbbm{1}-X\beta=\begin{pmatrix}
y_1-\beta_0-\displaystyle\sum_{k=1}^px_{1,k}\beta_k\\
\vdots\\
y_N-\beta_0-\displaystyle\sum_{k=1}^px_{N,k}\beta_k
\end{pmatrix}
\end{align*}
であることから
\begin{align*}
-\frac{1}{N}W(y-\beta_0\cdot\mathbbm{1}-X\beta)&=-\frac{1}{N}\begin{pmatrix}
\displaystyle\sum_{j=1}^Nw_{1,j}y_j-\beta_0\sum_{j=1}^Nw_{1,j}-\sum_{j=1}^Nw_{1,j}\sum_{k=1}^px_{j,k}\beta_k\\
\vdots\\
\displaystyle\sum_{j=1}^Nw_{N,j}y_j-\beta_0\sum_{j=1}^Nw_{N,j}-\sum_{j=1}^Nw_{N,j}\sum_{k=1}^px_{j,k}\beta_k
\end{pmatrix}
\end{align*}
が成立する必要がある．(\ref{eru})式が$(\hat{\beta},\hat{\beta}_0)$で最小となるには$\displaystyle\frac{\partial L_0}{\partial \beta_0\cdot\mathbbm{1}}={\bm 0}$となる必要があるので，最小値を与える$\hat{\beta}_0$が満たす条件は

\begin{align*}
\hat{\beta}_0\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}&=\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}y_j-\sum_{i=1}^N \sum_{j=1}^Nw_{i,j}\sum_{k=1}^px_{j,k}\beta_k\\
\therefore \hat{\beta}_0&=\frac{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}y_j}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}-\frac{\sum_{i=1}^N \sum_{j=1}^Nw_{i,j}\sum_{k=1}^px_{j,k}\beta_k}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}\\
&=\bar{y}-\frac{\sum_{k=1}^p\sum_{i=1}^N \sum_{j=1}^Nw_{i,j}x_{j,k}\beta_k}{\sum_{i=1}^N\sum_{j=1}^Nw_{i,j}}\\
&=\bar{y}-\sum_{k=1}^p \bar{X}_k\hat{\beta}_k=0
\end{align*}
となることがわかる．\\

以上の中心化により，改めて$\beta_0=0$とした上で
$$L(\beta):=\frac{1}{2N}(y-X\beta)^TW(y-X\beta)+\lambda\|\beta\|_1$$
の最小化を考えるが，これは$W=M^TM$とCholesky分解すれば，$V:=MX,u=My$とおくことにより
\begin{align*}
L(\beta)&=\frac{1}{2N}(y-X\beta)^T(M^T M)(y-X\beta)+\lambda\|\beta\|_1\\
&=\frac{1}{2N}((y-X\beta)^TM^T)(M(y-X\beta))+\lambda\|\beta\|_1\\
&=\frac{1}{2N}(My-MX\beta)^T(My-MX\beta)+\lambda\|\beta\|_1\\
&=\frac{1}{2N}\|u-V\beta\|_2^2+\lambda\|\beta\|_1
\end{align*}
となるため，結局通常の線形回帰におけるLasso
$$L(\beta)=\frac{1}{2N}\|u-V\beta\|_2^2+\lambda\|\beta\|_1$$
に帰着することができる．\\








\end{document}