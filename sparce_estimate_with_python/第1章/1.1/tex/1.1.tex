\documentclass{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{breqn}
\usepackage{amsthm}
\theoremstyle{definition}
%\newtheorem{Ex}{演習問題}
%\newtheorem*{Ex*}{演習問題}
\usepackage{latexsym}
\usepackage{emathEy}
\def\qed{\hfill$\Box$}
\usepackage{bm}
\pagestyle{myheadings}
\renewcommand{\footnotesize}{\normalsize}

\begin{document}
\large
\subsection*{スパースな状況($p>N$)における問題}
\begin{enumerate}[・]
\item 線形回帰で最小二乗法の解が求まらない；与えられた行列$X\in\mathbb{R}^{N\times p}$に対し，解を求めるのに必要な$X^t X$の逆行列が存在しない．\\

\item 情報量基準を用いて複数の変数を探すのが困難；変数が多すぎると，推定値がランダムな挙動に対して過敏になりすぎてしまう．また，推定値を定める変数を選ぶにも$p$個の変数からの選び方($=2^p通り)$を比較する必要がある．
\end{enumerate}
\subsection*{これらを解決するための方策}
\begin{enumerate}[・]
\item 二乗誤差の最小ではなく，それに係数の値が大きくなりすぎないようにするための正規化項を加えた関数の最小化問題を考える．これによって変数のノイズによって値が大きく変動してしまうことを防ぐ\\

\item 正規化項としては係数の$L1$ノルムの定数$\lambda$倍であるLasso,係数の$L2$ノルムの定数$\lambda$倍であるRidgeとよぶ．\\
(どうしてこれらについて考えるか→ Lasso ,Ridgeは共に凸関数の和であることから凸関数となり，最小化問題について考えやすくなる)\\

\item Lassoがモデル選択の役割を持つ→Lassoでは$\lambda$を大きくすることで特定の係数を$0$にすることができ，それによって注目すべき説明変数を絞ることができる

\end{enumerate}



\section*{考えている状況}
$N$個のデータ
\begin{align*}
(x_{1,1},&\cdots,x_{1,p},y_1)\\
(x_{2,1},&\cdots,x_{2,p},y_2)\\
&\vdots\\
(x_{N,1},&\cdots,x_{N,p},y_N)
\end{align*}
が与えられており，これらを基に$y$の値を${\bm x}\in\mathbb{R}^p$で推定したい．\\
$\Rightarrow $各$j$に対して$\beta\in\mathbb{R}^p$,$\beta_0\in\mathbb{R}$を用いて
$$\hat{y_j}:=\beta_0+\beta_1x_{1,j}+\cdots+\beta_px_{p,j}=\beta_0+\langle \beta,{\bm x} \rangle$$
によって$y$の値を推定する．このとき，実測値$y$と推定値$\hat{y}$との差
\begin{align*}
y-\hat{y}&=\left(
\begin{array}{c}
y_1\\
\vdots\\
y_N
\end{array}\right)-\left(\begin{array}{c}
\beta_0\\
\vdots\\
\beta_0
\end{array}\right)-\begin{pmatrix}
x_{1,1}&\cdots & x_{1,p}\\
\vdots & \ddots & \vdots\\
x_{N,1}& \cdots & x_{N,p}
\end{pmatrix}\left(\begin{array}{c}
\beta_1\\
\vdots\\
\beta_p
\end{array}\right)\\
&=y-{\bm \beta}_0-X\beta
\end{align*}
の$L_2$ノルム\footnote{$L_2$ノルムにすることで関数を滑らかにし，微分ができるようにしておく}の2乗を最小にする切片$\beta_0$と傾き$\beta$を求める．\\

まず各$j$に対して，$X$の第$j$列と$y$が中心化されているとする；\\
$X=\begin{pmatrix}
x_{1,1}&\cdots & x_{1,p}\\
\vdots & \ddots & \vdots\\
x_{N,1}& \cdots & x_{N,p}
\end{pmatrix}$と$y=\left(
\begin{array}{c}
y_1\\
\vdots\\
y_N
\end{array}\right)$に対して


\begin{align*}
X'&=\begin{pmatrix}
x_{1,1}-\bar{x_1}&\cdots & x_{1,p}-\bar{x_p}\\
\vdots & \ddots & \vdots\\
x_{N,1}-\bar{x_1}& \cdots & x_{N,p}-\bar{x_p}
\end{pmatrix}\quad \left(\bar{x_j}=\sum_{i=1}^N x_{i,j}\right)\\
y'&=\left(
\begin{array}{c}
y_1-\bar{y}\\
\vdots\\
y_N-\bar{y}
\end{array}\right)\quad \left(\bar{y}=\frac{1}{N}\sum_{i=1}^N y_i\right)
\end{align*}
と変形されており，
\begin{align*}
\bar{x'}_j&:=\frac{1}{N}\sum_{i=1}^N (x_{i,j}-\bar{x}_j)=0\quad \forall j\\
\bar{y}'&:=\frac{1}{N}\sum_{i=1}^N (y_i-\bar{y})=0
\end{align*}
となっているとする．このとき，$X'$と$y'$に関する最小二乗法の解$(\hat{\beta_0},\hat{\beta})$のうち，$\hat{\beta_0}$は$0$であることがわかる．

\begin{proof}
$\|y' -{\bm \beta}_0-X'\beta\|^2=\sum_{i=1}^N \left(y'_i-\beta_0-\sum_{j=1}^px_{i,j}\beta_j\right)^2$であり，この関数は$\beta_0^2$の係数が$1$の$\beta_0$の	二次式であるので
$$\|y' -{\bm \beta}_0-X'\beta\|^2が\hat{\beta_0}において最小\Rightarrow \frac{\partial}{\partial \beta}\|y' -{\bm \hat{\beta}_0}-X'\beta\|=0$$
が成立する．したがって


\begin{align*}
0&= \frac{\partial}{\partial \beta}\sum_{i=1}^N \left(y'_i-\beta_0-\sum_{j=1}^px'_{i,j}\beta_j\right)^2=\sum_{i=1}^N\frac{\partial}{\partial \beta}\left(y'_i-\beta_0-\sum_{j=1}^px'_{i,j}\beta_j\right)^2\\
&=\sum_{i=1}^N 2\left(y'_i-\beta_0-\sum_{i=1}^Nx'_{i,j}\beta_j\right)\cdot (-1)=-2\left(\sum_{i=1}^Ny_i-N\beta_0-\sum_{i=1}^N\sum_{j=1}^px'_{i,j}\beta_j\right)\\
&=-2N\left(\bar{y'}-\beta_0-\sum_{j=1}^p\frac{1}{N}\sum_{i=1}^N x'_{i,j}\beta_j\right)=-2N\left(\bar{y'}-\beta_0-\sum_{j=1}^p\bar{x'}_j\beta_j\right)\\
&=2N \beta_0\qquad \therefore \beta_0=0
\end{align*}\footnote{中心化された$X',y'$に関する最小二乗法の解$(\hat{\beta}_0,\hat{\beta})$が元の$X,y$に関する解になるかは要証明．}
\end{proof}

以下では$X,y$が中心化されているとする．このとき，切片$\beta_0=0$より$\|y -{\bm \beta}_0-X\beta\|^2=\sum_{i=1}^N (y_i-\sum_{j=1}^p x_{i,j}\beta_j)^2$となる．この式は各$j$に対して$\beta_j^2$の係数が$1$の$\beta_j$の二次式なので
$$\|y -X\beta\|^2が\hat{\beta}において最小\Rightarrow 各\beta_jに対して\frac{\partial}{\partial \beta_j}\|y-X\hat{\beta}\|^2=0$$
が成立する．また各$j$に対して
\begin{align*}
\frac{\partial}{\partial \beta_j}\|y-X\beta\|^2&=\frac{\partial}{\partial \beta_j}\sum_{i=1}^N \left(y_i-\sum_{j=1}^p x_{i,j}\beta_j\right)^2\\
&=\sum_{i=1}^N\frac{\partial}{\partial \beta_j} \left(y_i-\sum_{j=1}^p x_{i,j}\beta_j\right)^2\\
&=\sum_{i=1}^N 2\left(y_i-\sum_{k=1}^p x_{i,k}\beta_k\right)\cdot (-x_{i,j})\\
&=-2\sum_{i=1}^N x_{i,j}\left(y_i-\sum_{k=1}^p x_{i,k}\beta_k\right)
\end{align*}
であることから



\begin{align*}
({\bm 0}=)\left(\begin{array}{c}
\frac{\partial}{\partial \beta_1}\|y-X\hat{\beta}\|^2\\
\vdots\\
\frac{\partial}{\partial \beta_p}\|y-X\hat{\beta}\|^2
\end{array}\right)&=-2\left(\begin{array}{c}
\sum_{i=1}^N x_{i,1}\left(y_i-\sum_{k=1}^p x_{i,k}\beta_k\right)\\
\vdots\\
\sum_{i=1}^N x_{i,p}\left(y_i-\sum_{k=1}^p x_{i,k}\beta_k\right)
\end{array}\right)\\
&=-2\begin{pmatrix}
x_{1,1}& \cdots & x_{N,1}\\
\vdots & \ddots & \vdots\\
x_{N,1} & \cdots & x_{N,p}
\end{pmatrix}\left(\begin{array}{c}
y_1-\sum_{k=1}^p x_{1,k}\beta_k\\
\vdots\\
y_N-\sum_{k=1}^p x_{N,k}\beta_k\end{array}\right)\\
&=-2X^t(y-X\hat{\beta})
\end{align*}
が得られる．したがって正方行列$X^tX$が正則であるなら
\begin{align*}
X^t(y-X\hat{\beta})=0&\Leftrightarrow X^tX\hat{\beta}=X^t y\\
&\Leftrightarrow \hat{\beta}=(X^tX)^{-1}X^t y
\end{align*}

となり，最小二乗法の解が求まる．特に$p=1$のとき，$X=\begin{pmatrix}
x_1\\
\vdots\\
x_N
\end{pmatrix}$とすると$(X\neq O)$
\begin{align*}
\hat{\beta}&=\frac{\sum_{i=}^N x_iy_i}{\sum_{i=1}^N x_i^2}=\frac{\sum_{i=}^N x_iy_i/N}{\sum_{i=1}^N x_i^2/N}
\end{align*}
となることがわかる．\footnote{つまり変数が１つなら，最小二乗法の解$\beta$は$\frac{Xとyの共分散}{Xの分散}$で与えられる．}\\

以上より，中心化された$X,y$に対しては最小二乗法の解$(\hat{\beta}_0,\hat{\beta})$が$(0,(X^tX)^{-1}X^t y)$となることがわかった．中心化されていない$X,y$に対しても，式変形によって中心化されたものに帰着することができる．
\begin{proof}\mbox{}\\
$\bar{X}=\begin{pmatrix}
\bar{x}_1 & \cdots & \bar{x}_p\\
\vdots & \ddots & \vdots\\
\bar{x_1} & \cdots & \bar{x}_p
\end{pmatrix}\in\mathbb{R}^{N\times p},\bar{{\bm y}}=\begin{pmatrix}
\bar{y}\\
\vdots\\
\bar{y}
\end{pmatrix}\in \mathbb{R}^N$とすると

\begin{align*}
\|y-{\bm \beta_0}-X\beta\|^2&=\|y-\bar{{\bm y}}+\bar{{\bm y}}-{\bm \beta}_0 -(X-\bar{X})\beta-\bar{X}\beta\|^2\\
&=\|y'-(\bar{X}\beta-\bar{{\bm y}}+{\bm \beta}_0)-X'\beta\|^2
\end{align*}
であり，最右辺は中心化された$X',y'$に関する最小二乗法であるので
\begin{align*}
最右辺が最小\Rightarrow \bar{X}\beta-\bar{{\bm y}}+{\bm \beta}_0=0かつ \beta=({X'}^tX')^{-1}{X'}^t y'\\
\Leftrightarrow  \beta_0=\bar{y}-\sum_{i=1}^p x_i\beta_iかつ\beta=({X'}^tX')^{-1}{X'}^ty'
\end{align*}
であることから$X,y$に関する最小二乗法の解が$(\bar{y}-\sum_{i=1}^p x_i\beta_i,({X'}^tX')^{-1}{X'}^t y')$となることがわかる．
\end{proof}

上では$X^TX$が正則であることを仮定したが，一般に$N\times p$行列$X$に対して，$N<p$であれば$p$次正方行列$X^TX$は正則でない
\begin{proof}
行列$X$が定める線形写像を$f:\mathbb{R}^p\to\mathbb{R}^N$，$X$の転置行列$X^T$が定める線形写像を$f':\mathbb{R}^N\to \mathbb{R}^p$とおく．このとき，$X^T X$が正則となるためには合成写像$f'\circ f$に対して${\rm rank}(f'\circ f)=p$となる必要がある．\\
今，包含関係
\begin{align*}
{\rm Im}f(\mathbb{R}^p)\subset \mathbb{R}^N
\end{align*}
より
$${\rm Im}(f' (f(\mathbb{R}^N)))={\rm Im}(f'\circ f)\subset {\rm Im}f'$$
が成立することから
$${\rm rank}(f'\circ f)\leq {\rm rank}f'$$
がわかり，これと転置行列のランクが元の行列のランクに等しいことから
\begin{align}
\label{daiichi}
{\rm rank}(f'\circ f)\leq {\rm rank}f
\end{align}
が得られる．さらに写像$f$について，次元定理より
\begin{equation}
\begin{split}
\label{minip}
p=\dim \mathbb{R}^p&={\rm rank}f+{\rm null} f\\
\therefore {\rm rank}f&=p-{\rm null}f\leq p
\end{split}
\end{equation}
が成立し，また包含関係${\rm Im}f(\mathbb{R}^p)\subset \mathbb{R}^N$
より
\begin{align}
\label{minin}
{\rm rank}f\leq \dim \mathbb{R}^N= N
\end{align}
がわかる．したがって(\ref{minip}),(\ref{minin})式より
$${\rm rank}f\leq \min\{N,p\}\leq N<p$$
が成立するのでこれと(\ref{daiichi})式より
$${\rm rank}(f'\circ f)\leq {\rm rank}f<p$$
となるので${\rm rank}(f'\circ f)<p$がわかる．したがって行列$X^TX$は正則でないことが示せる．\\
\end{proof}
また，$X$に同じ列が2個ある場合にも$(\exists i,j\in\{1,\cdots,p\}\;s.t.; i\neq j\land x_{k,i}=x_{k,j}\;(\forall k\in \{1,\cdots, N\}))$，${\rm rank}X<p$となるので${\rm rank}X^TX<p$となり，逆行列が存在しないことがわかる．　\\
さらに変数の個数$p$の値が大きいと，目的変数$y$を説明するための説明変数を選ぶ際に$2^p$通りの変数の組み合わせを考える必要があり，計算量が膨大になる．\\

クロスバリデーション：いくつかのデータを分割して推定し，適切なモデルを選択する\\
(上の線形回帰での例)
\begin{enumerate}[(1)]
\item $2^p$通りの説明変数の組み合わせそれぞれに番号を振り，$s$番目の組み合わせを選んだとする．(その時の変数は$t$個であったとする)\\

\item 与えられた$N$個のデータを$k$個に分割する(分割したデータ族の$i$番目を$A_i(1\leq i\leq k)$とする)．\\

\item $\cup_{i\neq1}^kA_i$のデータに対して先ほどの手法で最小化問題を解く．\\

\item 得られた解$(\beta_{0,1},\beta_{1})$を用いた$A_1$における誤差を求める；$A_1$が
\begin{align*}
(x_{a_1,1},&\cdots,x_{a_1,t},y_{a_1})\\
&\vdots\\
(x_{a_s,1},&\cdots, x_{a_s,t},y_{a_s})
\end{align*}
となっているときに，誤差
\begin{align*}
e_{s,1}:=\left\|\begin{pmatrix}
y_{a_1}\\
\vdots\\
y_{a_s}
\end{pmatrix}-\begin{pmatrix}
\beta_{0,1}\\
\vdots\\
\beta_{0,1}
\end{pmatrix}-\begin{pmatrix}
x_{a_1,1}&\cdots&x_{a_1,s}\\
\vdots & \ddots & \vdots\\
x_{a_s,1}&\cdots,&x_{a_s,t}
\end{pmatrix}\begin{pmatrix}
\beta_{1,1}\\
\cdots\\
\beta_{1,s}
\end{pmatrix}\right\|^2
\end{align*}
を求める．\\

\item (3),(4)で$\cup_{i\neq 2}A_i,\cup_{i\neq 3}A_i,\cdots$の場合にも同様に誤差$e_{s,2},e_{s,3}$を求め，その平均値
$$e_{s}:=\frac{1}{k}\sum_{i=1}^Ne_{t,i}$$
を求める．\\

\item 以上を全ての$s$について行い，$e_s$が最も小さくなるような$s$を選ぶ．\\
\end{enumerate}

$p$が大きくなるにつれてこのような問題が生じてくる．これらを解決するために，以降では定数$\lambda\geq0$に対して，$\beta$の各成分が大きくなるごとに対する罰則を$\|y-X\beta\|^2$に加えた
\begin{align*}
L&:=\frac{1}{2N}\|y-X\beta\|^2+\lambda\|\beta\|_1\\
もしくは&\\
L&:=\frac{1}{N}\|y-X\beta\|^2+\lambda\|\beta\|_2
\end{align*}
の値を最小にする$\beta$を求める問題を検討する．ここで，$\hat{\beta}$が求まれば$\hat{\beta}_0$は求めることができたので，中心化された$X,y$に対して上式を最小にする$\hat{\beta}$を求め，それから$\hat{\beta}_0$を求めることにする．







\end{document}