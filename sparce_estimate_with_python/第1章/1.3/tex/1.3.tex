\documentclass{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{breqn}
\usepackage{amsthm}
\theoremstyle{definition}
\usepackage{bm}
%\newtheorem{Ex}{演習問題}
%\newtheorem*{Ex*}{演習問題}
\newtheoremstyle{mystyle}%   % スタイル名
    {}%                      % 上部スペース
    {}%                      % 下部スペース
    {\normalfont}%           % 本文フォント
    {}%                      % インデント量
    {\bf}%                   % 見出しフォント
    {}%                      % 見出し後の句読点, '.'
    { }%                     % 見出し後のスペース, ' ' or \newline
    {\underline{\thmname{#1}\thmnumber{#2}\thmnote{（#3）}}}%
                             % 見出しの書式 (can be left empty, meaning `normal')
\theoremstyle{mystyle} % スタイルの適用
\newtheorem*{Def}{Def}
\newtheorem*{theo}{Theorem}
\newtheorem*{lem}{Lemma}
\newtheorem*{ex}{Example}
\newtheorem*{col}{Corollary}
\renewcommand{\footnotesize}{\normalsize}
\usepackage{latexsym}
\usepackage{emathEy}
\def\qed{\hfill$\Box$}

\begin{document}
\large
\section*{Lasso}
1.1節で述べたように
\begin{align}
\tag{1.5}
L := \frac{1}{2N}\|y-X\beta\|^2+\lambda \|\beta\|_1
\end{align}
を最小にする問題をLassoという．まず簡単のために最初に
\begin{align}
\label{kantan}
\frac{1}{N}\sum_{i=1}^N x_{i,j}x_{i,k} =\begin{cases}
1 & j=k\\
0 & j\neq k
\end{cases}
\end{align}
を仮定する．この仮定は$N\times p$行列の各列の2乗平均が$1$(あるいは中心化されている$X$に対し，各列が標準化)かつ異なる列が直交するということに他ならない．つまり，各$i$に対して${\bm x}_i$を縦ベクトルとして
$$X = ({\bm x}_i,\cdots ,{\bm x}_p)$$
と書いたときに，$\|{\bm x}_i\|^2=N$かつ$\langle{\bm x}_i, {\bm x}_j \rangle=0(i\neq j)$が成立している状況を仮定している．\\
さらに$s_j = \displaystyle\frac{1}{N}\sum_{i=1}^N x_{i,j} y_i$とおく．これは$X$の第$j$列と$y$との内積とみることができる．こうすることで計算が容易になる．\\

$L$の定義より，各$j$に対して
$$L = \left(\beta_j^2の係数が\sum_{i=1}^Nx_{i,j}^2の2次式\right)+\lambda|\beta_j|+(\beta_j^0の項)$$
となり，凸関数の和で表されることから4ページ目の結果より$\beta_j$に関する凸関数になる．したがって
$$L(\beta_1,\cdots,\beta_p)が \beta_j において最小\Rightarrow Lの \beta_j に関する劣微分が0を含む$$
が成立するので，そのような$\beta_j$を求める．\\
$L$の$\beta_j$に関する劣微分を求めると，微分可能な凸関数に対して劣微分が微分係数と一致することから1ページ目の計算を用いて
\begin{align*}
(Lの\beta_jに関する劣微分) &=-\frac{1}{N}\sum_{i=1}^N x_{i,j}\left(y_i-\sum_{k=1}^p x_{i,k}\beta_k\right)+ \lambda\begin{cases}
1 & \beta_j>0\\
[-1,1] & \beta_j = 0\\
-1 & \beta_j< 0
\end{cases}\\
&=-\frac{1}{N}\left(\sum_{i=1}^N x_{i,j}y_i +\sum_{k=1}^p\sum_{i=1}^Nx_{i,j}x_{i,k} \beta_k\right)+\lambda\begin{cases}
1 & \beta_j>0\\
[-1,1] & \beta_j = 0\\
-1 & \beta_j< 0
\end{cases}\\
&=\begin{cases}
-s_j + \beta_j + \lambda & \beta_j>0\\
-s_j + \beta_j+\lambda[-1,1] & \beta_j = 0\\
-s_j + \beta_j  -\lambda & \beta_j<0
\end{cases}
\end{align*}
となることがわかる．したがって最右辺を$\beta_j$について解くと
\begin{align*}
\beta_j =\begin{cases}
s_j-\lambda & \beta_j>0\Leftrightarrow s_j-\lambda >0\\
0 & \beta_j = 0\Leftrightarrow -s_j\in [-\lambda,\lambda]\\
s_j + \lambda & \beta_j<0\Leftrightarrow s_j+\lambda<0
\end{cases}
\end{align*}
が得られる．上式は$\mathbb{R}$上の関数
\begin{align}
\tag{1.11}
\mathcal{S}_{\lambda}(x) &:=\begin{cases}
x-\lambda & x>\lambda\\
0 & |x|\leq \lambda\\
x+\lambda & x<-\lambda
\end{cases}
\end{align}
を用いることで$\beta_j =\mathcal{S}_{\lambda}(s_j)$とかくことができる．つまり(1.9)式の仮定の下では，(1.5)式を最小にする$\hat{\beta}\in\mathbb{R}^p$は定数$s_1,\cdots,s_p$を用いて
\begin{align*}
\hat{\beta} = 
\begin{pmatrix}
\hat{\beta_1}\\
\hat{\vdots}\\
\hat{\beta_p}
\end{pmatrix}=\begin{pmatrix}
\mathcal{S}_{\lambda}(s_1)\\
\vdots\\
\mathcal{S}_{\lambda}(s_p)
\end{pmatrix}
\end{align*}
とかくことができるのである．\\

ここで，$\mathbb{R}$上の関数$\mathcal{S}'_{\lambda}(x)$を
\begin{align*}
\mathcal{S}'_{\lambda}(x) := {\rm sgn}(x)\cdot \max\{|x|-\lambda,0\}\quad(ただし{\rm sgn}は符号関数)
\end{align*}
によって定義すると，この関数は(1.11)式で定義された関数$\mathcal{S}_{\lambda}(x)$と一致する．著書のプログラムはこの事実を用いて書かれている．\\


次に，仮定が無い場合にはどうなるか考える．まず$X,y$は中心化されているとする．このとき，Lの$\beta_j$における劣微分を考えると
\begin{equation}
\begin{split}
\label{nagai}
(Lの\beta_jに関する劣微分) &=-\frac{1}{N}\sum_{i=1}^N x_{i,j}\left(y_i-\sum_{k=1}^p x_{i,k}\beta_k\right)+ \lambda\begin{cases}
1 & \beta_j>0\\
[-1,1] & \beta_j = 0\\
-1 & \beta_j< 0
\end{cases}\\
&=-\frac{1}{N}\sum_{i=1}^Nx_{i,j}\left(y_i-\left(x_{i,j} \beta_j+\sum_{k\neq j}x_{i,k}\beta_k\right)\right)+\lambda\begin{cases}
1 & \beta_j>0\\
[-1,1] & \beta_j = 0\\
-1 & \beta_j< 0
\end{cases}\\
&=\frac{1}{N}\sum_{i=1}^N(x_{i,j})^2\beta_j-\frac{1}{N}\sum_{i=1}^Nx_{i,j}\left(y_i-\sum_{k\neq j}x_{i,k}\beta_k\right)+\lambda\begin{cases}
1 & \beta_j>0\\
[-1,1] & \beta_j = 0\\
-1 & \beta_j< 0
\end{cases}\\
\end{split}
\end{equation}
となることがわかる．ここで，最右辺第2項は教科書の通り$\displaystyle r_{i,j} = y_i-\sum_{k\neq k}x_{i,k}\beta_k$とすることで$s_j := \frac{1}{N}\sum_{i=1}^Nx_{i,j}r_{i,j}$と表される．さらに第1項について，$\displaystyle\frac{1}{N}\sum_{i=1}^N(x_{i,j})^2$が$1$であれば$\beta_j$について解くことは安易なのでこれを仮定する．つまり，今$X$は
\begin{align*}
X = \displaystyle\begin{pmatrix}
\displaystyle\frac{x_{1,1}-\bar{x}_1}{\sigma_1} & \cdots & \displaystyle\frac{x_{1,p}-\bar{x}_p}{\sigma_p}\\
\vdots & \ddots & \vdots\\
\displaystyle\frac{x_{N,1}-\bar{x}_1}{\sigma^1}& \cdots &\displaystyle \frac{x_{N,p}-\bar{x}_p}{\sigma^p}
\end{pmatrix},\quad \sigma_j = \sqrt{\frac{1}{N}\sum_{i=1}^N(x_{i,j}-\bar{x}_j)^2}
\end{align*}
と書かれていると仮定する．このことは$X$の各列が標準化されていることに他ならない．\\

上の仮定のもとでは，(\ref{nagai})式は$\beta_j$について解くことができて
\begin{align}
\label{bii}
\beta_j = \begin{cases}
s_j -\lambda&(s_j>\lambda)\\
0 &(|s_j|\leq \lambda)\\
s_j+\lambda & (s_j<-\lambda)
\end{cases}=\mathcal{S}_{\lambda}(s_j)
\end{align}
となることがわかる．\\

ここで注意すべきは，上式の右辺は$\beta_k(k\neq j)$を用いて表していることである．すなわち，$\beta_k(k\neq j)$を固定した下で$L$を最小にする$\beta_j$は求めることができるが，$L$を最小にする$\beta\in\mathbb{R}^p$を一度に求めるのは上の手法では不可能である．そこで，以下に述べる座標降下法という手法を用いて$\beta$を求めることにする．

\subsection*{座標降下法}\mbox{}\\
以下では，$L$を$\beta_j$に関する1変数関数$L(\beta_j)$とみている時には$L_j$とかくことにする．
\begin{enumerate}[(i)]
\item $\beta\in\mathbb{R}^p$を任意にとる．(解$\hat{\beta}$の近くであれば収束は速いが，一般には定まらないので教科書では初期値${\bm 0}$をとっている．)このときとった$\beta$の各成分を$\beta_{1,1},\cdots,\beta_{p,1}$と書くことにする(つまり$\beta=(\beta_{1,1},\cdots,\beta_{p,1})$)．\\

\item $L=L(\beta_1,\cdots,\beta_p)$に$\beta_2=\beta_{2,1},\beta_3 = \beta_{3,1},\cdots,\beta_p=\beta_{p,1}$を代入することで，$L$を$\beta_1$に関する$1$変数関数$L(\beta_1)$とする．\\

\item (ii)で生成した$L_1$を最小にする$\beta_1$を(\ref{bii})式を解いて求める．そこで得られた解を$\beta_1'$とおく．このとき，
\begin{align}
\label{ichi}
L(\beta_1',\beta_{2,1},\cdots,\beta_{p,1})\leq L(\beta)
\end{align}
となる．\\

\item 次に$L$に$\beta_1=\beta_1',\beta_3 = \beta_{3,1},\cdots,\beta_p=\beta_{p,1}$を代入して1変数$L_2$を生成する．\\

\item (iv)で生成した$L_2$を最小にする$\beta_2$を(\ref{bii})式を解いて求める．そこで得られた解を$\beta_2'$とおく．このとき
\begin{align}
\label{nii}
L(\beta_1',\beta_2',\cdots,\beta_{p,1})\leq L(\beta_1',\beta_{2,1},\cdots,\beta_{p,1})
\end{align}
となる．\\

\item 上で行った操作を$p$までについて行う．すなわち，$\beta_{j-1}'\;(j\leq p)$まで得られているとき，$L$に$\beta_1 = \beta_1',\cdots,\beta_{j-1}= \beta_{j-1}',\beta_{j+1}=\beta_{j+1,1},\cdots,\beta_p = \beta_{p,1}$を代入して$\beta_j$に関する1変数関数$L_j$を生成し，$L_j$を最小にする$\beta_j$を(\ref{bii})式を解いて求め，得られた解を$\beta_j'$として$\beta_{j+1}'$を求める．このとき
\begin{align}
\label{takussan}
L(\beta_1',\cdots,\beta_{j-1}',\beta_j',\beta_{j+1,1},\cdots,\beta_{p,1})\leq L(\beta_1',\cdots,\beta_{j-1}',\beta_{j,1},\beta_{j+1,1},\cdots,\beta_{p,1})
\end{align}
となることに注意する．

\item (vi)により新たに$\beta':=(\beta_1',\beta_2',\cdots,\beta_p')$が得られる．このとき，(\ref{ichi}),(\ref{nii}),(\ref{takussan})式より
$$ L(\beta)\geq L(\beta')$$
が成立することがわかる．したがって$\beta'$を新たに初期値として(ii)からの操作を施すというループを無限回行うことで
\begin{align*}
L(\beta)\geq L(\beta')\geq L(\beta'')\geq\cdots\geq L(\beta^{(n)})\geq \cdots
\end{align*}
を満たす$\mathbb{R}$上の数列$\{L(\beta^{(n)}\}_{n=1}^{\infty}$が得られる．任意の自然数$n$について
$$L(\beta^{(n)})\geq \inf_{\beta\in\mathbb{R}^p}L(\beta)\geq 0$$
が成立することから数列$\{L(\beta^{(n)})\}_{n=1}^{\infty}$は下に有界な単調減少列であるので極限$\displaystyle\lim_{n\to\infty}L(\beta^{(n)})$が存在し，それは$ \displaystyle\inf_{\beta\in\mathbb{R}^p}L(\beta)$に一致する．この極限を$\hat{\beta}$とすれば，$\forall \beta\in \mathbb{R}$に対して
\begin{align*}
L(\beta)\geq L(\hat{\beta})
\end{align*}
となるので，求める$\hat{\beta}$を生成することができる．\qed\\
\end{enumerate}

上記の手順で$L$を最小にする$\hat{\beta}$を生成できるが，それには無限試行という手順が必要なので現実には有限回の操作で打ち切って近似値を求めることになる．\\

$\mathbb{R}$上の収束列$\{L(\beta^{(n)})\}_{n=1}^{\infty}$は特に$\mathbb{R}$上のCauchy列であることから，任意の$\varepsilon>0$に対して十分大きな自然数$m,n$が存在して
$$|L(\beta^{(m)})-L(\beta^{(n)})|<\varepsilon$$
が成立する．したがって十分大きな$m$のもとで
\begin{equation}
\begin{split}
\label{kannkai}
|L(\beta^{(m)})-L(\hat{\beta})|&=\lim_{n\to\infty}|L(\beta^{(m)})-L(\beta^{(n)})|\\
&\leq |L(\beta^{(m)})-L(\beta^{(m+1)})|+\lim_{n\to\infty}|L(\beta^{(m+1)}-L(\beta^{(n)})|\\
&< |L(\beta^{(m)})-L(\beta^{(m+1)})|+\varepsilon
\end{split}
\end{equation}
となることから$ |L(\beta^{(m)})-L(\beta^{(m+1)})|$の差が小さくなるまでループを繰り返すことで任意の精度の近似値が得られる．あるいは$L$が($\mathbb{R}^p$と$\mathbb{R}$に通常の位相が入った上で)連続関数となることから，任意の$\varepsilon>0$に対して
\begin{align*}
\|\beta^{(m)}-\beta^{(m+1)}\|<\delta\Rightarrow |L(\beta^{(m)})-L(\beta^{(m+1)})|<\varepsilon
\end{align*}
を満たすような$\delta>0$が存在する．これと(\ref{kannkai})式を考えれば，$\|\beta^{(m)}-\beta^{(m+1)}\|$の差が小さくなるまでループを繰り返すことで精度の良い近似値を得られることができる．著書では後者を採用している．\\

以上より各列が標準化された$X$と，中心化された$y$について$L$を最小にする$\hat{\beta}$を求めることができた．これを用いることで一般の$X$について$L$を最小にする$\hat{\beta},\hat{\beta}_0$を求める；\\

$X$中心化したものを$\overline{X}$，標準化したものを$X'$とかく．さらに$p$次正方行列$C$を
\begin{align*}
C=\displaystyle\begin{pmatrix}
\frac{1}{\sigma_1} & \cdots & \frac{1}{\sigma_p}\\
\vdots & \cdots & \vdots\\
\frac{1}{\sigma_1}& \cdots & \frac{1}{\sigma_p}
\end{pmatrix}
\end{align*}
で定める．このとき$X'=\overline{X}C$であることに注意すれば
\begin{align*}
\frac{1}{2N}\|y-X'\beta\|^2 +\lambda\|\beta\|_1が\hat{\beta}において最小
&\Leftrightarrow  \frac{1}{2N}\|y-\overline{X}\beta\|^2 +\lambda\|\beta\|_1が \beta = C\hat{\beta}において最小
\end{align*}
となることがわかるので，中心化した$X$,$y$について$L$を最小にする$\beta$が
$$\hat{\beta}'=C\hat{\beta} = \begin{pmatrix}
\hat{\beta}_1/\sigma_1\\
\displaystyle\vdots\\
\hat{\beta}_p/\sigma_p
\end{pmatrix}$$
とかけることが示せた．あとは(1.4)式を用いることで，一般の$X,y$について
$L=\frac{1}{2N}\|y-X\beta-\beta_0\|^2+ \lambda\|\beta|_1$を最小にする$\hat{\beta},\hat{\beta}_0$は
$$\hat{\beta} =\hat{\beta}',\quad \hat{\beta}_0= \bar{y}-\sum_{i=1}^p\bar{x}_j\hat{\beta}'_j$$
となることがわかる．\qed\\

\end{document}