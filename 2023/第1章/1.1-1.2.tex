\documentclass{jsarticle}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{ascmac}
\usepackage{breqn}
\usepackage{amsthm}
\theoremstyle{definition}
%\newtheorem{Ex}{演習問題}
%\newtheorem*{Ex*}{演習問題}
\usepackage{latexsym}
\def\qed{\hfill$\Box$}
\pagestyle{myheadings}
\numberwithin{equation}{section}

\begin{document}
\large

\section{}
\subsection{}訓練集合として，$N$個の観測地$x$を並べた${\bm x}:=(x_1,\cdots,x_n)^T $とそれぞれ対応する観測値$t$を並べた${\bm t}:= (t_1,\cdots,t_N)^N$が与えられたとする．そして目標データ集合${\bm t}$は，まず$\sin(2\pi x)$の関数値を作成したのち，ガウス分布に従う小さなランダムノイズを加えて対応する$t_n$を作った．すなわち
$$t_n \sim sin(2\pi x_n) + \varepsilon \quad \varepsilon \sim N(0,\sigma^2)$$
となるデータを作成している．このようにして生成されたデータは多くの現実データ集合の持つ性質をよく表している．すなわち，データはこれから学習しようとする規則性を保持してはいるが，それぞれの観測値はランダムノイズによって不正確なものになっている．このノイズは，放射性崩壊のように，本質的に確率的なランダムプロセスによる場合もあるが，多くはそれ自身は観測されない信号源の変動によるものである．\\

我々の木費用は，この訓練集合を利用して，新たな入力変数の値$\hat{x}$に対して$\hat{t}$の値を予測することである．後で見るように，これは背後にある関数$\sin (2\pi x)$を暗に見つけようとすることとほぼ等価であるが，有限個のデータ集合から汎化しなければならない点で，本質的に難しい問題である．さらに，観測データはノイズが乗っており，与えられた$\hat{x}$に対する$\hat t$の値には不確実性がある．1.2説では，そのような不確実性を厳密かつ定量的に評価する枠組みを与える．また1.5説で議論する決定理論は，確率論的な枠組みを利用して，適切な基準の下での最適な予測をすることを可能にする．\\

ただしここでは話を先に進めるために，曲線フィッティングに基づく単純なアプローチを，あまり形式ばらない形で考えよう．ここでは特に，以下のような多項式を使ってデータへのフィッティングを行うことにする．
\begin{equation}
\begin{split}
y(x,{\bm w}) = w_0 + w_1x + w_2x^2 + \cdots + w_Mx^M = \sum_{j=0}^M w_j x^j
\end{split}
\end{equation}
ただし，$M$は多項式の次数で，$x^j$は$x$の$j$乗を表す．多項式$y(x,{\bm w})$は$x$の非線形関数であるものの，係数${\bm w}$の線形関数であることに注意する．すなわち，相異なる$x_1,x_2$に対して
$$y(x_1,{\bm w}) + y(x_2,{\bm w}) = y(x_1+x_2,{\bm w})$$
は常には成り立たないものの，相異なる${\bm w}_1,{\bm w}_2$に対しては
\begin{align*}
  y(x,{\bm w}_1) + y(x,{\bm w}_2) = y(x,{\bm w}_1 + {\bm w}_2)
\end{align*}
が成立する．多項式のように，未知のパラメータに関して線形であるような関数は非常に重要な性質を持つ．それらは線形モデルと呼ばれ，のちの章で詳細に議論する．\\
訓練データに多項式を当てはめることで係数の値を求めてみよう．これは${\bm w}$を任意に固定した時の関数$y(x,{\bm w})$の値と訓練集合のデータ点との間のずれを測る誤差関数の最小化で達成できる．誤差関数の選び方として，単純で広く用いられているのは，各データ点$x_n$における予測値$y(x_n,{\bm w})$と対応する目標値$t_n$との二乗和誤差
\begin{align}
  \label{gosa}
E({\bm w}) = \frac{1}{2}\sum_{n=1}^N\{y(x_n,{\bm w}) - t_n\}^2
\end{align}
となり，これを最小化することになる．のちにこの関数を選ぶ理由について議論するが，利点の1つは，これが微分可能な凸関数であることにある．また，上の関数が$0$となるのは$y(x,{\bm w}_n)$が全訓練データを通るとき，かつその時に限ることに注意する．\\


このように$E({\bm w})$をできるだけ小さくするような$\bm w$を選ぶことで曲線当てはめ問題を解くことができる．誤差関数は係数$\bm w$の二次関数だから，その係数に関する微分は$\bm w$の要素に関して線形になり，誤差関数を最小にするただ1つの解$\bm w^{\star}$が
$\varphi(x) := (x^0 ,x,x^2,\cdots,x^M)^T$
として
\begin{align*}
\bm w^{\star} = \left( \sum_{n=1}^N\varphi(x_n)\varphi(x_n)^T \right)^{-1}\left( \sum_{n=1}^N\varphi(x_n)^T t_n \right)
\end{align*}
と閉じた形で求まる．実際$y(x,\bm w) = \varphi(x)^T \bm w$に注意して(\ref{gosa})式を${\bm w}$で微分して$0$と置くと
\begin{align*}
\nabla E(\bm w) &= \sum_{n=1}^N\varphi(x_n)\{\varphi(x)^T \bm w-t_n\}\\
                &= \sum_{n = 1}^N (\varphi(x_n)\varphi(x_n)^Tw - \varphi(x_n)^T t_n) = 0\\
                \therefore \bm w &= (\varphi(x_n) \varphi(x_n)^T)^{-1}(\varphi(x_n)t_n)
\end{align*}
が得られる．また，これより結果として得られる多項式は$y(x,\bm w^{\star})$となる．\\

あとは多項式の次数$M$を選ぶ問題が残っているが，この問題はモデル比較(モデル選択)と呼ぶ重要な概念の一例とみなすことができる．\\

例に見る通り$M=0,1$の場合にはあまりデータへの当てはまりが良くない一方で，$M=3$の場合がこの中では$\sin(2\pi x)$にもっともよく当てはまっているように見える．しかし$M=9$にした場合，訓練データには非常によく当てはまっている(実際にこの多項式は全データを通っている)ものの曲線は無茶苦茶に発振したようになっており，関数$\sin(2\pi x)$の表現として明らかに不適切である．このような学習は過学習として知られている．\\

我々の目標は新たなデータに対して正確な予測を行える高い汎化性能を達成することである．汎化性能が次数$M$にどう依存するかを定量的に評価するため，100個のデータ点からなる独立したテスト集合を，訓練データとまったく同じ方法で生成する．すると，選んだ$M$の各値について(\ref{gosa})式で与えられる$E(\bm w^{\star})$の残渣が計算できるが，テスト集合についても$E(\bm w^{\star})$を評価できる．このとき，
\begin{equation}
\begin{split}
  E_{RMS}&=\sqrt{2E(\bm w^{\star}/N)}\\
          &= \sqrt{\frac{1}{N}\sum_{n=1}^N \{y(x_n,\bm w^{\star}) - t_n\}^2}
\end{split}
\end{equation}
で定義される平均二乗平方根誤差を用いると比較に便利なことがある．$N$で割ることによってサイズの異なるデータ集合を比較することができるようになり，平方根をとることによって$E_{RMS}$は目的変数$t$と同じ尺度であることが保障される．テスト集合の誤差は新たな$x$を観測した時に$t$をどれだけよく予測できたかを表している．例の通り，$M$が小さいと誤差が大きく，これは$\sin(2\pi x)$の振動をとらえることができないことを意味しており，$3\leq M\leq 8$では比較的誤差が小さく妥当な表現といえる．\\

$M=9$では訓練集合の誤差は$0$になる．しかし関数$u(x,\bm w^{\star})$の無茶苦茶な発振が起きるので，テスト集合の誤差は非常に大きくなる．$M=9$次の多項式が潜在的に$M=3$次の多項式を含むことを考えると，このことはパラドックスのように思えることもある．\\
さらに新たなデータに対する最良な予測関数はデータを生成した関数$\sin (2\pi x)$であると考えることもできる．後にこのことを示す．関数$\sin (2\pi x)$の級数展開は区間$(0,1)$ですべての次数の項を含むことより，$M$を増やせば増やすほど単調に良い結果が得られると期待してしまう．\\

いろいろな次数の多項式について得られた$\bm w^{\star}$の値を検証してみると，$M$の増加に伴って係数の多くが大きな値をとるようになることがわかる．これにより訓練集合のデータ点にはピッタリ適合するものの，データ点とデータ点の間では大きな発振が起こってしまう．\\

次にモデルの次数は固定し，データ集合のサイズを変えてみた時の振る舞いについて，モデルの複雑さを固定した時，データ集合のサイズが大きくなるにつれて過学習の問題は深刻ではなくなってくることがわかる．別な言い方をすると，データ集合を大きくすればするほど，より複雑で柔軟なモデルをデータにあてはめられるようになる．大雑把な経験則としては，データ点の数はモデル中の適応パラメータの数の何倍かよりは小さくてはならない，と言われている．しかし，3章で見るように，必ずしもパラメータの数がモデルの複雑さを測る最適な尺度というわけではない．\\

また入手できる訓練集合のサイズに応じてモデルのパラメータの数を制限する必要があるのは納得できない感もする．モデルの複雑さはデータ点の個数ではなく，解くべき問題の複雑さに応じて選ぶのがもっともに思える．最小二乗でモデルのパラメータを求めるアプローチが最尤推定の特別な場合に相当し，過学習の問題が最尤推定の持つ一般的性質として理解できることを後で示す．\\

過学習の問題を避けるにはベイズ的アプローチを採用すればよい．ベイズの観点からはモデルのパラメータ数がデータ点の数をはるかに超えても問題がないことが後にわかる．実際，ベイズモデルにおいては有効パラメータ数は自動的にデータ集合のサイズに適合する．\\

ベイズ的アプローチ以外で，複雑で柔軟なモデルを限られたサイズのデータ集合に対して使うことができるかを考える．過学習の現象を制御するためによく使われる手法として正則化がある．これは誤差関数に罰金項を付加することにより係数が大きな値になることを防ごうとするものである．そのようなもので最も単純な誤差関数は
\begin{align}
  \label{ridge}
\tilde E(\bm w) = \frac{1}{2}\sum_{n=1}^N\{y(x_n,\bm w)- t_n\}^2+\frac{\lambda}{2}\|w\|^2
\end{align}
である．ただし一般性を失うことなく$w_0 = 0$とすることができ，正則化からは外すことも多い．この場合も誤差関数(\ref*{ridge})式を最小にする解は
\begin{align*}
  \bm w^{\star} = \left( \sum_{n=1}^N\varphi(x_n)\varphi(x_n)^T + \lambda I\right)^{-1}\left( \sum_{n=1}^N\varphi(x_n)^T t_n \right)
  \end{align*}
と閉じた形で求まる．このようなテクニックは統計学の分野で縮小推定と呼ばれており，特に2次の場合はリッジ回帰と呼ばれる．またニューラルネットワークの分野では荷重減衰として知られている．\\

同じデータ集合に対し，$M=9$を当てはめた結果を見ると，過学習が抑制されて背後の$\sin (2\pi x)$にずっと近い表現が得られていることがわかる．しかしながら，$\lambda$を大きくし過ぎると当てはまりは再び悪くなる．\\

モデルの複雑さに関する議論は1.3節で詳しく議論するが，ここでは単に誤差関数を最小にするようなアプローチで実際の応用問題を解こうとする際には，モデルの複雑さを適切に決める方法を見つけなければならないということを注意しておく．得られたデータを係数決定のための訓練集合と，テストのための確認用集合の2つに分けるという単純な方法が思いつくが，この方法では貴重な訓練データを無駄にすることになることが多く，より洗練されたアプローチを探す必要がある．\\

\subsection{}
確率の例を単純な例を使って導入する．赤と青の2つの箱があり，赤い箱にはリンゴ2個とオレンジ6個．青の箱にはリンゴ3個とオレンジ1個入っているとする．赤い箱を40\%,青の箱を60\%で選び，箱の中の果物は分け隔てなく同じ確からしさで選ぶ．\\
この例ではどの箱を選ぶかを表すものが確率変数となり，今それを$B$で表すことにする．この変数は2つの可能な値，すなわち$r$(赤い箱)または$b$(青い箱)をとりうる．同様にどの果物かを表すのも確率変数であり，これを$F$で表すこれは$a$(リンゴ)か$o$(オレンジ)の値をとる．\\

事象の確率を，その事象が起きた回数と全試行回数の比で定義する．ただし，全試行回数が無限に多くなった時の極限を考える．このとき
$$p(B = r ) = \frac{4}{10},\quad p(B = b) = \frac{6}{10}$$

となる．事象の集合が互いに排反で，すべての可能な場合を含んでいれば，それらの事象の確率の総和は1になることが要請される．\\

このような状況で考えうる質問「リンゴを選び出す確率はいくらか」，「オレンジを選び出したとして，それが青い箱から取り出されたものである確率はいくつか」，あるいはパターン認識問題に関連したより込み入った質問にも答えるには，確率に関する2つの基本的な法則のみ知っていればよい．すなわち確率の加法定理と確率の乗法定理である．以下でこれらを導出することを考える．\\

2つの確率変数$X,Y$は任意の値$x_i(i = 1,\cdots,M),y_i(i = 1,\cdots,L)$をそれぞれとれるものとする．$X,Y$の両方についてサンプルを取り，全部で$N$回の試行を行う．そのうち$X = x_i,Y = y_j$となる試行の数を$n_{ij}$とする．また$X$が値$x_i$を取る試行の数を$c_i$とし，同様に$Y$が$y_j$を取る試行の数を$r_j$とする．$X$が$x_i$，$Y$が$y_j$を取る確率を$p(X = x_i,Y = y_j)$と書き，$X = x_i$と$Y = y_j$の同時確率と呼ぶ．この場合は
\begin{align}
\label{zenjishou}
p(X = x_i, Y = y_j) = \frac{n_{i,j}}{N}
\end{align}
で与えられる．同様に$X$が$c_i$を取る確率は
\begin{align}
\label{shuhen}
p(X = x_i) = \frac{c_i}{N}
\end{align}
で与えられる．これは$c_i = \sum_{j}n_{ij}$であることを用いれば
\begin{align}
  p(X = x_i ) = \sum_{j=1 }^Lp(X = x_i,Y = y_j)
\end{align}
が成立する．これが確率の加法定理である．この場合，$p(X = x_i)$を周辺確率と呼ぶこともある．\\

$X = x_i$の事例だけを考え，その中での$Y = y_j$の事例の比率を$p(Y = y_j\mid X = x_i)$と書き，$X = x_i$が与えられた下での$Y = y_j$の条件付き確率と呼ぶ．この場合は
\begin{align}
\label{jyouken}
p(Y = y_j\mid X = x_i) = \frac{n_{ij}}{c_i}
\end{align}
となる．(\ref{zenjishou}),(\ref{shuhen}),(\ref{jyouken})式より次の関係を得る．
\begin{equation}
\begin{split}
  p( X = x_i,Y = y_j) &=\frac{n_{ij}}{N}\\
  &=\frac{n_{ij}}{c_i}\cdot \frac{c_i}{N}\\
  &=p(Y = y_j\mid X = x_i)p(X = x_i)
\end{split}
\end{equation}
これは確率の乗法定理である．\\
まとめると，確率論の基本法則を以下のように書くことができる．
\begin{align}
{\rm 確率の加法定理}\qquad & p(X) = \sum_Y p(X,Y)\\
{\rm 確率の乗法定理} \qquad& p(X,Y) = p(Y\mid X)p(X)
\end{align}
乗法定理と対称性$p(X,Y) = p(Y,X)$より関係式
\begin{align}
p(Y\mid X) = \frac{p(X\mid Y)p(Y)}{p(X)}
\end{align}
を得る．これはベイズの定理と呼ばれ，パターン認識や機械学習において中心的な役割を果たす．また，上式の分母は
\begin{align}
p(X) = \sum_Y p(X\mid Y)p(Y)
\end{align}
と分子に現れる量を使って表すことができる．\\


果物の箱の例に戻る．赤，青の箱を選ぶ確率はそれぞれ
\begin{align}
p(B = r) = 4/10\\
p(B = b) = 6/10
\end{align}

選んだ箱が青い箱であった場合にリンゴを選ぶ確率は，単に青い箱の中のリンゴの個数の比率で$3/4$なので$p(F = a\mid B = b) = 3/4$である．実際に箱の種類が与えられた下での果物の条件付き確率をすべて書き出すと
\begin{align}
p(F = a\mid B = r) &= 1/4\\
p(F = o\mid B = r) &= 3/4\\
p(F = a\mid B = b) &= 3/4\\
p(F = o\mid B = b) &= 1/4
\end{align}
となり，これらの確率はすべて規格化されていることが確認できる．
\begin{align}
p(F = a\mid B = r) + p(F = o\mid B = r) &= 1\\
p(F = a\mid B = b) + p(F = o\mid B = b) &= 1
\end{align}
確率の加法定理，乗法定理を用いることでリンゴを選ぶ確率は
\begin{equation}
\begin{split}
p(F = a)&=p(F = a\mid B = r)p(B = r) + p(F = o\mid B = r)p(B = r)\\
&=\frac{1}{4} \times \frac{4}{10} + \frac{3}{4}\times \frac{6}{10} = \frac{11}{20}
\end{split}
\end{equation}
となり，また加法定理から$p(F = o) =\displaystyle 1-\frac{11}{20} = \frac{9}{11}$がいえる．一方果物が与えられたもとでの条件付確率はベイズの定理より




\end{document}


